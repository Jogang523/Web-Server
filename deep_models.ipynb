{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "url = r'C:\\Users\\조갱\\Downloads\\사용할거'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(url, 'train')\n",
    "test_dir = os.path.join(url, 'test')\n",
    "val_dir = os.path.join(url, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(train_dir)\n",
    "os.mkdir(test_dir)\n",
    "os.mkdir(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training directories\n",
    "train_flower_dir = os.path.join(train_dir, 'flower')\n",
    "train_market_dir = os.path.join(train_dir, 'market')\n",
    "train_mountains_dir = os.path.join(train_dir, 'mountains')\n",
    "# train_museum_dir = os.path.join(train_dir, 'museum')\n",
    "train_night_view_dir = os.path.join(train_dir, 'night_view')\n",
    "train_sea_dir = os.path.join(train_dir, 'sea')\n",
    "train_theme_park_dir = os.path.join(train_dir, 'theme_park')\n",
    "\n",
    "# Testing directories\n",
    "test_flower_dir = os.path.join(test_dir, 'flower')\n",
    "test_market_dir = os.path.join(test_dir, 'market')\n",
    "test_mountains_dir = os.path.join(test_dir, 'mountains')\n",
    "# test_museum_dir = os.path.join(test_dir, 'museum')\n",
    "test_night_view_dir = os.path.join(test_dir, 'night_view')\n",
    "test_sea_dir = os.path.join(test_dir, 'sea')\n",
    "test_theme_park_dir = os.path.join(test_dir, 'theme_park')\n",
    "\n",
    "# Validation directories\n",
    "val_flower_dir = os.path.join(val_dir, 'flower')\n",
    "val_market_dir = os.path.join(val_dir, 'market')\n",
    "val_mountains_dir = os.path.join(val_dir, 'mountains')\n",
    "# val_museum_dir = os.path.join(val_dir, 'museum')\n",
    "val_night_view_dir = os.path.join(val_dir, 'night_view')\n",
    "val_sea_dir = os.path.join(val_dir, 'sea')\n",
    "val_theme_park_dir = os.path.join(val_dir, 'theme_park')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(train_flower_dir)\n",
    "os.mkdir(train_market_dir)\n",
    "os.mkdir(train_mountains_dir)\n",
    "# os.mkdir(train_museum_dir)\n",
    "os.mkdir(train_night_view_dir)\n",
    "os.mkdir(train_sea_dir)\n",
    "os.mkdir(train_theme_park_dir)\n",
    "\n",
    "os.mkdir(test_flower_dir)\n",
    "os.mkdir(test_market_dir)\n",
    "os.mkdir(test_mountains_dir)\n",
    "# os.mkdir(test_museum_dir)\n",
    "os.mkdir(test_night_view_dir)\n",
    "os.mkdir(test_sea_dir)\n",
    "os.mkdir(test_theme_park_dir)\n",
    "\n",
    "os.mkdir(val_flower_dir)\n",
    "os.mkdir(val_market_dir)\n",
    "os.mkdir(val_mountains_dir)\n",
    "# os.mkdir(val_museum_dir)\n",
    "os.mkdir(val_night_view_dir)\n",
    "os.mkdir(val_sea_dir)\n",
    "os.mkdir(val_theme_park_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 폴더에서 첫 1500개 파일을 불러오는 코드\n",
    "flower_file = os.listdir(url + '/flower')[:363]\n",
    "market_file = os.listdir(url + '/market')[:408]\n",
    "mountains_file = os.listdir(url + '/mountains')[:398]\n",
    "# museum_file = os.listdir(url + '/museum')[:1500]\n",
    "night_view_file = os.listdir(url + '/night_view')[:335]\n",
    "sea_file = os.listdir(url + '/sea')[:442]\n",
    "theme_park_file = os.listdir(url + '/theme_park')[:435]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flower 이미지 복사\n",
    "for f in flower_file[:254]:\n",
    "    shutil.copy(url + '/flower/' + f, train_flower_dir)\n",
    "for f in flower_file[254:326]:\n",
    "    shutil.copy(url + '/flower/' + f, val_flower_dir)\n",
    "for f in flower_file[326:363]:\n",
    "    shutil.copy(url + '/flower/' + f, test_flower_dir)\n",
    "\n",
    "# market 이미지 복사\n",
    "for f in market_file[285]:\n",
    "    shutil.copy(url + '/market/' + f, train_market_dir)\n",
    "for f in market_file[285:366]:\n",
    "    shutil.copy(url + '/market/' + f, val_market_dir)\n",
    "for f in market_file[366:408]:\n",
    "    shutil.copy(url + '/market/' + f, test_market_dir)\n",
    "\n",
    "# mountains 이미지 복사\n",
    "for f in mountains_file[:278]:\n",
    "    shutil.copy(url + '/mountains/' + f, train_mountains_dir)\n",
    "for f in mountains_file[278:357]:\n",
    "    shutil.copy(url + '/mountains/' + f, val_mountains_dir)\n",
    "for f in mountains_file[357:398]:\n",
    "    shutil.copy(url + '/mountains/' + f, test_mountains_dir)\n",
    "\n",
    "# # museum 이미지 복사\n",
    "# for f in museum_file[:1000]:\n",
    "#     shutil.copy(url + '/museum/' + f, train_museum_dir)\n",
    "# for f in museum_file[1000:1300]:\n",
    "#     shutil.copy(url + '/museum/' + f, val_museum_dir)\n",
    "# for f in museum_file[1300:1500]:\n",
    "#     shutil.copy(url + '/museum/' + f, test_museum_dir)\n",
    "\n",
    "# night_view 이미지 복사\n",
    "for f in night_view_file[:234]:\n",
    "    shutil.copy(url + '/night_view/' + f, train_night_view_dir)\n",
    "for f in night_view_file[234:301]:\n",
    "    shutil.copy(url + '/night_view/' + f, val_night_view_dir)\n",
    "for f in night_view_file[301:335]:\n",
    "    shutil.copy(url + '/night_view/' + f, test_night_view_dir)\n",
    "\n",
    "# sea 이미지 복사\n",
    "for f in sea_file[:309]:\n",
    "    shutil.copy(url + '/sea/' + f, train_sea_dir)\n",
    "for f in sea_file[309:397]:\n",
    "    shutil.copy(url + '/sea/' + f, val_sea_dir)\n",
    "for f in sea_file[397:442]:\n",
    "    shutil.copy(url + '/sea/' + f, test_sea_dir)\n",
    "\n",
    "# theme_park 이미지 복사\n",
    "for f in theme_park_file[:304]:\n",
    "    shutil.copy(url + '/theme_park/' + f, train_theme_park_dir)\n",
    "for f in theme_park_file[304:392]:\n",
    "    shutil.copy(url + '/theme_park/' + f, val_theme_park_dir)\n",
    "for f in theme_park_file[392:442]:\n",
    "    shutil.copy(url + '/theme_park/' + f, test_theme_park_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#기본모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "train Loss: 0.3390 Acc: 0.9250\n",
      "val Loss: 0.1668 Acc: 0.9443\n",
      "Epoch 2/10\n",
      "train Loss: 0.0373 Acc: 0.9945\n",
      "val Loss: 0.1211 Acc: 0.9580\n",
      "Epoch 3/10\n",
      "train Loss: 0.0183 Acc: 0.9983\n",
      "val Loss: 0.0932 Acc: 0.9683\n",
      "Epoch 4/10\n",
      "train Loss: 0.0126 Acc: 0.9987\n",
      "val Loss: 0.0854 Acc: 0.9713\n",
      "Epoch 5/10\n",
      "train Loss: 0.0083 Acc: 0.9991\n",
      "val Loss: 0.0898 Acc: 0.9680\n",
      "Epoch 6/10\n",
      "train Loss: 0.0063 Acc: 0.9998\n",
      "val Loss: 0.0801 Acc: 0.9720\n",
      "Epoch 7/10\n",
      "train Loss: 0.0048 Acc: 0.9995\n",
      "val Loss: 0.0803 Acc: 0.9720\n",
      "Epoch 8/10\n",
      "train Loss: 0.0038 Acc: 1.0000\n",
      "val Loss: 0.0949 Acc: 0.9667\n",
      "Epoch 9/10\n",
      "train Loss: 0.0036 Acc: 0.9998\n",
      "val Loss: 0.0993 Acc: 0.9653\n",
      "Epoch 10/10\n",
      "train Loss: 0.0033 Acc: 0.9998\n",
      "val Loss: 0.0791 Acc: 0.9710\n",
      "Test Acc: 0.9850\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 전처리 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.ToTensor(),  # 텐서로 변환\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 데이터 로드\n",
    "data_dir = r'C:\\Users\\조갱\\Downloads\\사용할거'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f'{data_dir}/{x}', transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "# 3. 모델 정의 (ResNet18 사용)\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # 10개의 클래스로 분류\n",
    "\n",
    "# 4. 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 5. 학습 루프\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):  # 에포크 수\n",
    "    print(f'Epoch {epoch+1}/10')\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # 학습 모드\n",
    "        else:\n",
    "            model.eval()   # 평가 모드\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 데이터 반복\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(image_datasets[phase])\n",
    "        epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# 6. 테스트 데이터로 평가\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_acc.double() / len(image_datasets['test'])\n",
    "print(f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#resnet 50 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "train Loss: 0.3287 Acc: 0.9365\n",
      "val Loss: 0.1337 Acc: 0.9610\n",
      "Epoch 2/10\n",
      "train Loss: 0.0220 Acc: 0.9972\n",
      "val Loss: 0.1095 Acc: 0.9677\n",
      "Epoch 3/10\n",
      "train Loss: 0.0125 Acc: 0.9983\n",
      "val Loss: 0.1028 Acc: 0.9720\n",
      "Epoch 4/10\n",
      "train Loss: 0.0073 Acc: 0.9995\n",
      "val Loss: 0.0859 Acc: 0.9747\n",
      "Epoch 5/10\n",
      "train Loss: 0.0051 Acc: 0.9995\n",
      "val Loss: 0.1179 Acc: 0.9673\n",
      "Epoch 6/10\n",
      "train Loss: 0.0045 Acc: 0.9996\n",
      "val Loss: 0.1103 Acc: 0.9733\n",
      "Epoch 7/10\n",
      "train Loss: 0.0032 Acc: 0.9998\n",
      "val Loss: 0.1119 Acc: 0.9740\n",
      "Epoch 8/10\n",
      "train Loss: 0.0028 Acc: 0.9999\n",
      "val Loss: 0.0787 Acc: 0.9780\n",
      "Epoch 9/10\n",
      "train Loss: 0.0025 Acc: 0.9998\n",
      "val Loss: 0.0685 Acc: 0.9803\n",
      "Epoch 10/10\n",
      "train Loss: 0.0024 Acc: 0.9998\n",
      "val Loss: 0.1132 Acc: 0.9733\n",
      "Test Acc: 0.9825\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 전처리 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.ToTensor(),  # 텐서로 변환\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 데이터 로드\n",
    "data_dir = r'C:\\Users\\조갱\\Downloads\\사용할거'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f'{data_dir}/{x}', transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "# 3. 모델 정의 (ResNet50 사용)\n",
    "model = models.resnet50(pretrained=True)  # ResNet50으로 변경\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # 10개의 클래스로 분류\n",
    "\n",
    "# 4. 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 5. 학습 루프\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):  # 에포크 수\n",
    "    print(f'Epoch {epoch+1}/10')\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # 학습 모드\n",
    "        else:\n",
    "            model.eval()   # 평가 모드\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 데이터 반복\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(image_datasets[phase])\n",
    "        epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# 6. 테스트 데이터로 평가\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_acc.double() / len(image_datasets['test'])\n",
    "print(f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, r\"C:\\Users\\조갱\\Documents\\GitHub\\Web-Server\\Deep_Model\\Models\\EfficientNet_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VGG19 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 전처리 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.ToTensor(),  # 텐서로 변환\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 데이터 로드\n",
    "data_dir = r'C:\\Users\\조갱\\Downloads\\사용할거'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f'{data_dir}/{x}', transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "# 3. 모델 정의 (VGG19 사용)\n",
    "model = models.vgg19(pretrained=True)  # VGG19으로 변경\n",
    "num_ftrs = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_ftrs, 10)  # 10개의 클래스로 분류\n",
    "\n",
    "# 4. 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 5. 학습 루프\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):  # 에포크 수\n",
    "    print(f'Epoch {epoch+1}/10')\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # 학습 모드\n",
    "        else:\n",
    "            model.eval()   # 평가 모드\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 데이터 반복\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(image_datasets[phase])\n",
    "        epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# 6. 테스트 데이터로 평가\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_acc.double() / len(image_datasets['test'])\n",
    "print(f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EfficientNet 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to C:\\Users\\조갱/.cache\\torch\\hub\\checkpoints\\efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 24.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "train Loss: 0.8558 Acc: 0.8475\n",
      "val Loss: 0.2618 Acc: 0.9547\n",
      "Epoch 2/10\n",
      "train Loss: 0.1193 Acc: 0.9845\n",
      "val Loss: 0.1487 Acc: 0.9627\n",
      "Epoch 3/10\n",
      "train Loss: 0.0580 Acc: 0.9892\n",
      "val Loss: 0.1085 Acc: 0.9687\n",
      "Epoch 4/10\n",
      "train Loss: 0.0381 Acc: 0.9942\n",
      "val Loss: 0.1220 Acc: 0.9620\n",
      "Epoch 5/10\n",
      "train Loss: 0.0265 Acc: 0.9959\n",
      "val Loss: 0.1074 Acc: 0.9657\n",
      "Epoch 6/10\n",
      "train Loss: 0.0208 Acc: 0.9964\n",
      "val Loss: 0.0946 Acc: 0.9707\n",
      "Epoch 7/10\n",
      "train Loss: 0.0149 Acc: 0.9981\n",
      "val Loss: 0.1017 Acc: 0.9673\n",
      "Epoch 8/10\n",
      "train Loss: 0.0147 Acc: 0.9974\n",
      "val Loss: 0.0836 Acc: 0.9707\n",
      "Epoch 9/10\n",
      "train Loss: 0.0119 Acc: 0.9985\n",
      "val Loss: 0.0913 Acc: 0.9690\n",
      "Epoch 10/10\n",
      "train Loss: 0.0109 Acc: 0.9981\n",
      "val Loss: 0.0724 Acc: 0.9740\n",
      "Test Acc: 0.9975\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 전처리 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # EfficientNet은 224x224 입력 크기\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.ToTensor(),  # 텐서로 변환\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 데이터 로드\n",
    "data_dir = r'C:\\Users\\조갱\\Downloads\\사용할거'\n",
    "image_datasets = {x: datasets.ImageFolder(root=f'{data_dir}/{x}', transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "# 3. 모델 정의 (EfficientNet 사용)\n",
    "model = models.efficientnet_b0(pretrained=True)  # EfficientNet-B0 불러오기\n",
    "num_ftrs = model.classifier[1].in_features  # 마지막 분류 계층의 입력 피쳐 수 확인\n",
    "model.classifier[1] = nn.Linear(num_ftrs, 10)  # 10개의 클래스로 분류\n",
    "\n",
    "# 4. 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 5. 학습 루프\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):  # 에포크 수\n",
    "    print(f'Epoch {epoch+1}/10')\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # 학습 모드\n",
    "        else:\n",
    "            model.eval()   # 평가 모드\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # 데이터 반복\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(image_datasets[phase])\n",
    "        epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "# 6. 테스트 데이터로 평가\n",
    "model.eval()\n",
    "test_acc = 0.0\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    test_acc += torch.sum(preds == labels.data)\n",
    "\n",
    "test_acc = test_acc.double() / len(image_datasets['test'])\n",
    "print(f'Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Name: Gyeongbokgung_Palace\n",
      "63_Building: 11.84% 확률\n",
      "Bulguksa: 0.98% 확률\n",
      "Changdeokgung_Palace: 1.93% 확률\n",
      "Gwangan_Bridge: 16.98% 확률\n",
      "Gyeongbokgung_Palace: 35.38% 확률\n",
      "Lotte_World_Tower: 19.71% 확률\n",
      "Myeongdong_Cathedral: 0.65% 확률\n",
      "NamJunePaik_Art_Center: 7.12% 확률\n",
      "Namsan_Tower: 4.32% 확률\n",
      "Yeongnamnu: 1.09% 확률\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\조갱\\AppData\\Local\\Temp\\ipykernel_20848\\2394215782.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(r\"C:\\Users\\조갱\\Documents\\GitHub\\Web-Server\\Deep_Model\\Models\\EfficientNet_model.pth\", map_location=torch.device('cpu'))  # CPU 또는 GPU에 맞게 설정\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models  # datasets 임포트 추가\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 소프트맥스 함수 사용\n",
    "from PIL import Image\n",
    "import os  # os 모듈 임포트 추가\n",
    "\n",
    "# 1. 데이터 전처리 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 이미지 크기 조정\n",
    "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
    "        transforms.ToTensor(),  # 텐서로 변환\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 2. 데이터 로드 (임의의 데이터 경로 설정)\n",
    "data_dir = r'C:\\Users\\조갱\\Downloads\\사용할거'\n",
    "image_datasets = {x: datasets.ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}\n",
    "\n",
    "# 3. 새로운 데이터 전처리 (이미지 예측용)\n",
    "new_data_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. 모델 불러오기\n",
    "# 저장된 모델을 전체적으로 불러옵니다.\n",
    "loaded_model = torch.load(r\"C:\\Users\\조갱\\Documents\\GitHub\\Web-Server\\Deep_Model\\Models\\EfficientNet_model.pth\", map_location=torch.device('cpu'))  # CPU 또는 GPU에 맞게 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()  # 평가 모드로 설정\n",
    "\n",
    "# 5. 단일 이미지 예측 함수 정의\n",
    "def predict_image_with_confidence(image_path, model, transform):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():  # 학습이 아니므로 gradient를 계산하지 않음\n",
    "        output = model(image)\n",
    "        probabilities = F.softmax(output, dim=1)  # 소프트맥스 적용하여 확률 계산\n",
    "        _, predicted_class = torch.max(probabilities, 1)  # 가장 확률 높은 클래스 예측\n",
    "\n",
    "    return predicted_class.item(), probabilities\n",
    "\n",
    "# 6. 예측 및 결과 출력\n",
    "image_path = r\"C:\\Users\\조갱\\Downloads\\다리4.jpg\"  # 예측하려는 이미지 경로\n",
    "predicted_class_idx, probabilities = predict_image_with_confidence(image_path, loaded_model, new_data_transform)\n",
    "\n",
    "# 클래스 이름 가져오기 (image_datasets['train'].classes에서 클래스 목록을 얻습니다)\n",
    "class_names = image_datasets['train'].classes\n",
    "predicted_class_name = class_names[predicted_class_idx]\n",
    "print(f\"Predicted Class Name: {predicted_class_name}\")\n",
    "\n",
    "# 7. 각 클래스별 확률 출력\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {probabilities[0, idx].item()*100:.2f}% 확률\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "63_Building: 0.89% 확률\n",
    "Bulguksa: 0.17% 확률\n",
    "Changdeokgung_Palace: 4.97% 확률\n",
    "Gwanggan_Bridge: 8.33% 확률\n",
    "Gyeongbokgung_Palace: 83.22% 확률\n",
    "Lotte_World_Tower: 0.82% 확률\n",
    "Myeongdong_Cathedral: 0.11% 확률\n",
    "NamJunePaik_Art_Center: 0.70% 확률\n",
    "Namsan_Tower: 0.67% 확률\n",
    "Yeongnamnu: 0.12% 확률"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
